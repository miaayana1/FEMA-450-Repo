# -*- coding: utf-8 -*-
"""finalprojectmia_avellanet.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1RVpUgIxzorzO_5QglpPNUdcFJgxjtzlg

## INFO 450 Final Project: FEMA Disaster Relief

*Mia Avellanet*
"""

import pandas as pd
import numpy as np
import plotly.express as px
from scipy import stats

DATA_URL = "https://storage.googleapis.com/info_450/IndividualAssistanceHousingRegistrantsLargeDisasters%20(1).csv"

df=pd.read_csv(DATA_URL, nrows=10000)
df.head(5)

df=df.dropna(subset=["tsaEligible"])
df["tsaEligible"]=df["tsaEligible"].astype(int)

# Drops rows where tsaEligible (target) is missing"""

df["residenceType"]=df["residenceType"].fillna("Unknown")

# Fills missing residenceType with Unknown"""

df["grossIncome_missing"]=df["grossIncome"].isna().astype(int)
df["grossIncome"]=df["grossIncome"].fillna(0)
df["repairAmount_missing"]=df["repairAmount"].isna().astype(int)
df["repairAmount"]=df["repairAmount"].fillna(0)

# Fills missing grossIncome and repairAmount with 0, and creates flags"""

df["damagedStateAbbreviation"]=df["damagedStateAbbreviation"].fillna("UNK")

# Fills missing state abbreviations with "UNK"
 for col in ["destroyed","specialNeeds"]:
  if col in df.columns:
    df[col]=df[col].replace(
        {"Yes":1, "yes": 1, "No": 0, "no": 0}
    ).fillna(0).astype(int) 

crosstab_state=pd.crosstab(df["residenceType"],df["tsaEligible"], normalize="index")*100
#print("\nTSA Eligibility Rate by State/Territory(%):")
#print(crosstab_state.round(1))

# TSA Eligibility by residence type"""

crosstab_state=pd.crosstab(df["damagedStateAbbreviation"],df["tsaEligible"], normalize="index")*100
#print("\nTSA Eligibility Rate by State/Territory(%):")
#print(crosstab_state.round(1))

# TSA eligibility by state/territory"""

avg_repair=df.groupby("damagedStateAbbreviation")["repairAmount"].mean().sort_values(ascending=False)
#print("\nAverage Repair Amount by State:")
#print(avg_repair.head(10))

# Average repair amount by state"""

tsa_rate_state=(
    df.groupby("damagedStateAbbreviation")["tsaEligible"]
    .mean()
    .sort_values(ascending=False)
    .reset_index()
)
figone=px.bar(
    tsa_rate_state,
    x="damagedStateAbbreviation",
    y="tsaEligible",
    title="TSA Eligibility Rate by State/Territory",
    labels={"tsaEligible": "Eligibility Rate", "damagedStateAbbreviation": "State"},
)
figone.show()

# Bar Chart: TSA eligibility rate by state"""

figtwo=px.histogram(
    df,
    x="repairAmount",
    nbins=60,
    title="Distribution of Repair Amount",
    labels={"repairAmount": "Repair Amount ($)"},
)
figtwo.show()

# Histogram: Distribution of repairAmount"""

figthree=px.box(
    df,
    x="residenceType",
    y="repairAmount",
    title="Repair Amount Across Residence Types",
    labels={"residenceType": "Residence Type", "repairAmount": "Repair Amount ($)"},
)
figthree.show()

# Boxplot: repairAmount across residence types"""

figfour=px.histogram(
    df,
    x="specialNeeds",
    color="tsaEligible",
    barmode="group",
    title="TSA Eligibility by Special Needs Status",
    labels={"specialNeeds": "Special Needs (0=No, 1=Yes)", "tsaEligible": "TSA Eligible"},
)
figfour.show()

def mean_confidence_interval(data, confidence=0.95):
  data=np.array(data)
  n = len(data)
  mean = np.mean(data)
  se = stats.sem(data)
  h = se * stats.t.ppf((1+confidence)/2, n-1)
  return mean, mean - h, mean + h

# Defines a small function in order to calculate for CI"""

mean_all, lower_all, upper_all = mean_confidence_interval(df["repairAmount"])
#print("Overall Mean Repair Amount and 95% Confidence Interval:")
#print(f"Mean = ${mean_all:,.2f}")
#print(f"95% CI = [${lower_all:,.2f}, ${upper_all:,.2f}]")

# Calculate CI for repairAmount (whole sample)"""

eligible=df[df["tsaEligible"]==1]["repairAmount"]
not_eligible=df[df["tsaEligible"]==0]["repairAmount"]

#I calculated a 95& CI for the average repair amount to estimate the range where the true population mean falls. This gives FEMA an estimation of the typical repair costs applicants faced after disasters. The confidence interval shows the average repair amount along with upper and lower limits, meaning that we can be 95% confident that the actual mean repair cost is within that range."""

t_stat,p_value = stats.ttest_ind(eligible,not_eligible,equal_var=False)
#t_stat, p_value

# Welch's t-test"""

#print("\nT-Test: TSA Eligible vs Not Eligible")
#print(f"t-statistic = {t_stat:.3f}")
#print(f"p-value = {p_value:.4f}")

if p_value < 0.05:
  print("There is a significant difference in average repair amounts")
else:
  print("There is no significant difference in average repair amounts")

#I used a t-test to compare the average repair amounts between applicants who were TSA eligible and those who were not. This test checks if the difference in their averages is statistically significant or just due to random chance. If the p-value was less than 0.05, it meant that the difference is significant, which could correlate to TSA eligibility possiblity relating to higher/lower repair costs."""

state1="LA"
state2="TX"
la=df[df["damagedStateAbbreviation"]==state1]["repairAmount"]
tx=df[df["damagedStateAbbreviation"]==state2]["repairAmount"]

t_stat2,p_value2 = stats.ttest_ind(tx,la,equal_var=False)

print(f"\nT-Test: {state1} vs {state2}")
print(f"t-statistic = {t_stat2:.3f}")
print(f"p-value = {p_value2:.4f}")

if p_value2 < 0.05:
    print(f" {state1} and {state2} have significantly different average repair amounts.")
else:
    print(f" No significant difference between {state1} and {state2}.")

#I ran another t-test in order to compare the mean repair amounts between applicants in LA and TX. Both states experience frequent hurricanes and floods, so I was curious to see if the damage costs differ. If the p-value was below 0.05, it shows a meaningful difference in average repair amounts between the two states. If it was higher, it means their repair costs were not significantly different. In my case, LA and TX are significantly difference because of the p value being 0. These results sugguest that both eligibility and location play an important role in the amount of repair assistance applicants may need."""

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import MinMaxScaler, OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.pipeline import Pipeline
from sklearn.metrics import accuracy_score, precision_score, recall_score, confusion_matrix, classification_report

X=df[["grossIncome", "repairAmount", "destroyed","waterLevel","residenceType", "damagedStateAbbreviation"]]
y=df["tsaEligible"].astype(int)

# Before modeling, I filled any missing values to avoid errors and ensure all rows are used. Numeric values such as grossIncome and repairAmount were replaced with 0, and text fields were replaced with "unknown" or UNK"""

X["grossIncome"] = X["grossIncome"].fillna(0)
X["repairAmount"] = X["repairAmount"].fillna(0)
X["destroyed"] = X["destroyed"].fillna(0)
X["waterLevel"] = X["waterLevel"].fillna(0)
X["residenceType"] = X["residenceType"].fillna("Unknown")
X["damagedStateAbbreviation"] = X["damagedStateAbbreviation"].fillna("UNK")

# I chose predictors that could influence if an applicant qualifies for TSA. The variable tsaEligible indicates if an applicant was approved for temporary shelter assistance."""

X_train, X_test, y_train, y_test=train_test_split(X,y,test_size=0.2,random_state=42)

# The dataset was split into 80% training and 20% testing to evaluate model performance fairly."""

numeric_features=["grossIncome","repairAmount", "destroyed", "waterLevel"]
categorical_features=["residenceType","damagedStateAbbreviation"]

preprocessor=ColumnTransformer(
    transformers=[
        ("num",MinMaxScaler(),numeric_features),
        ("cat",OneHotEncoder(handle_unknown="ignore"),categorical_features)
    ]
)

# I used MinMaxScaler to scale numeric variables between 0 and 1 so that no feature dominates the others. For the text variables, I used OneHotEncoder to convert them into binary columns."""

tree_pipeline=Pipeline(steps=[
    ("preprocessor",preprocessor),
    ("model", DecisionTreeClassifier(max_depth=6, random_state=42))
])

rf_pipeline = Pipeline(steps=[
    ("preprocessor", preprocessor),
    ("model", RandomForestClassifier(n_estimators=150, max_depth=10, random_state=42))
])

#* I built a decision tree and a random forest. The random forest combines many trees to improve accuracy. The decision tree provides a visual of the key predictors. Both use the same preprocessing pipeline for consistency."""

tree_pipeline.fit(X_train, y_train)
rf_pipeline.fit(X_train, y_train)

y_pred_tree=tree_pipeline.predict(X_test)
y_pred_rf=rf_pipeline.predict(X_test)

#After training I used each model to predict TSA eligibility for the test set"""

def show_results(name, y_true, y_pred):
  print(f"\n{name} Results:")
  print("Accuracy:", round(accuracy_score(y_true, y_pred),3))
  print("Precision:", round(precision_score(y_true, y_pred),3))
  print("Recall:", round(recall_score(y_true,y_pred),3))
  print("Confusion Matrix:\n", confusion_matrix(y_true,y_pred))

show_results("Decision Tree", y_test,y_pred_tree)
show_results("Random Forest",y_test,y_pred_rf)

# I then evaluated both models using accuracy, precision, recall, and confusion matrices. I think the random forest model performed better in regards to all the metris, because it showed stronger generalization and less missclassifications. As a result of this, I think the Random Forest model is the better one for predicting TSA eligibility."""

#!pip install streamlit
import streamlit as st
import pandas as pd
import plotly.express as px

st.title("FEMA Disaster Relief Dashboard")
st.subheader("Data Preview")
st.write(df.head())

#histogram of repair amount
st.subheader("Histogram of Repair Amount")
fig_hist=px.histogram(df, x="repairAmount",nbins=30,title="Distribution of Repair Amounts")

st.plotly_chart(fig_hist)

#boxplot of repair amount by tsa eligibility
st.subheader("Boxplot: Repair Amount by TSA Eligibility")
fig_box=px.box(
    df, 
    x="tsaEligible", 
    y="repairAmount",
    color="tsaEligible",
    title="Repair Amount by TSA Eligibilty",
    labels={"tsaEligible": "TSA Eligible (1=Yes,0=No)", "repairAmount":"Repair Amount"}
)
st.plotly_chart(fig_box)

st.markdown("*Insight:* Compare the central tendency and spread of repair amounts for TSA eligible vs. non-eligible households.*")
